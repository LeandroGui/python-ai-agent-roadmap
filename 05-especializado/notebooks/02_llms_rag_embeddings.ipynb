{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBtBGLhPRMLf",
        "outputId": "311d12da-4189-472f-9943-d0f2e84c0460"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m53.3/53.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m234.9/234.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires google-auth==2.43.0, but you have google-auth 2.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "# InstalaciÃ³n de librerÃ­as necesarias\n",
        "!pip install -q -U google-genai scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MÃ³dulos\n",
        "\n",
        "from google import genai\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# --- CONFIGURACIÃ“N API Key ---\n",
        "# En un entorno real, usar os.environ.get(\"GOOGLE_API_KEY\")\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEYL')\n",
        "\n",
        "client = genai.Client(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "AfgZYOgZUm5k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modelos disponibles en Gemini\n",
        "print(\"List of models that support generateContent:\\n\")\n",
        "for m in client.models.list():\n",
        "    for action in m.supported_actions:\n",
        "        if action == \"generateContent\":\n",
        "            print(m.name)\n",
        "\n",
        "print(\"List of models that support embedContent:\\n\")\n",
        "for m in client.models.list():\n",
        "    for action in m.supported_actions:\n",
        "        if action == \"embedContent\":\n",
        "            print(m.name)\n",
        "\n",
        "print(\"\\nâœ… LibrerÃ­as importadas y entorno configurado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l6ljHeeU8fd",
        "outputId": "38b76552-10d0-4943-f7ad-7b5bca671128"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List of models that support generateContent:\n",
            "\n",
            "models/gemini-2.5-flash\n",
            "models/gemini-2.5-pro\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-exp-image-generation\n",
            "models/gemini-2.0-flash-lite-001\n",
            "models/gemini-2.0-flash-lite\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.5-flash-preview-tts\n",
            "models/gemini-2.5-pro-preview-tts\n",
            "models/gemma-3-1b-it\n",
            "models/gemma-3-4b-it\n",
            "models/gemma-3-12b-it\n",
            "models/gemma-3-27b-it\n",
            "models/gemma-3n-e4b-it\n",
            "models/gemma-3n-e2b-it\n",
            "models/gemini-flash-latest\n",
            "models/gemini-flash-lite-latest\n",
            "models/gemini-pro-latest\n",
            "models/gemini-2.5-flash-lite\n",
            "models/gemini-2.5-flash-image-preview\n",
            "models/gemini-2.5-flash-image\n",
            "models/gemini-2.5-flash-preview-09-2025\n",
            "models/gemini-2.5-flash-lite-preview-09-2025\n",
            "models/gemini-3-pro-preview\n",
            "models/gemini-3-flash-preview\n",
            "models/gemini-3-pro-image-preview\n",
            "models/nano-banana-pro-preview\n",
            "models/gemini-robotics-er-1.5-preview\n",
            "models/gemini-2.5-computer-use-preview-10-2025\n",
            "models/deep-research-pro-preview-12-2025\n",
            "List of models that support embedContent:\n",
            "\n",
            "models/embedding-001\n",
            "models/text-embedding-004\n",
            "models/gemini-embedding-exp-03-07\n",
            "models/gemini-embedding-exp\n",
            "models/gemini-embedding-001\n",
            "\n",
            "âœ… LibrerÃ­as importadas y entorno configurado.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ§  Cortex Nivel 5: LLMs y RAG (GeneraciÃ³n Aumentada)\n",
        "\n",
        "AquÃ­ conectamos a Cortex con **Google Gemini**. Pero la memoria de un LLM es estÃ¡tica (se entrenÃ³ hace meses). Para que sepa sobre datos privados o recientes, usamos **RAG (Retrieval Augmented Generation)**.\n",
        "\n",
        "## Conceptos Clave\n",
        "\n",
        "### 1. Embeddings (Vectores SemÃ¡nticos)\n",
        "Las computadoras no entienden palabras, entienden nÃºmeros.\n",
        "Un \"Embedding\" convierte texto en una lista de nÃºmeros (vector).\n",
        "* *Ejemplo:* \"Perro\" y \"Caniche\" tendrÃ¡n vectores matemÃ¡ticamente cercanos. \"Perro\" y \"Tostadora\" estarÃ¡n lejos.\n",
        "\n",
        "### 2. BÃºsqueda SemÃ¡ntica\n",
        "En lugar de buscar por palabras clave exactas (Ctrl+F), buscamos por significado usando la **Similitud del Coseno**.\n",
        "\n",
        "### 3. El Prompt del Agente\n",
        "No solo le decimos \"responde\". Le damos una **Persona** (\"Eres un experto acadÃ©mico\") y **Herramientas** (\"Usa el contexto provisto\")."
      ],
      "metadata": {
        "id": "ci8GXwIDRXEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. MOTOR DE EMBEDDINGS (Hugging Face o Gemini) ---\n",
        "# Usaremos Gemini para simplificar, pero el concepto es igual con HF\n",
        "def get_embedding(text):\n",
        "    result = client.models.embed_content(\n",
        "        model=\"models/text-embedding-004\",\n",
        "        contents=[text]) # contents expects a list of strings\n",
        "\n",
        "    [embedding_obj] = result.embeddings\n",
        "\n",
        "    return embedding_obj.values\n",
        "\n",
        "# --- 2. MINI BASE VECTORIAL (Memoria RAG) ---\n",
        "knowledge_base = [\n",
        "    \"La fecha del examen final de Python es el 15 de Diciembre.\",\n",
        "    \"El profesor de AnÃ¡lisis de Datos se llama Leandro.\",\n",
        "    \"Para aprobar se necesita un promedio mayor a 70.\"\n",
        "]\n",
        "\n",
        "# Convertimos texto a vectores\n",
        "vector_db = [get_embedding(doc) for doc in knowledge_base]\n",
        "\n",
        "# --- 3. BÃšSQUEDA SEMÃNTICA ---\n",
        "query = \"Â¿CuÃ¡ndo rindo y quÃ© nota necesito?\"\n",
        "query_vector = get_embedding(query)\n",
        "\n",
        "# Calculamos similitud (MatemÃ¡tica vectorial)\n",
        "# Reshape necesario porque sklearn espera matrices\n",
        "similarities = cosine_similarity(\n",
        "    [query_vector],\n",
        "    vector_db\n",
        ")\n",
        "\n",
        "# Obtenemos el Ã­ndice del documento mÃ¡s similar\n",
        "best_idx = np.argmax(similarities)\n",
        "retrieved_info = knowledge_base[best_idx]\n",
        "\n",
        "print(f\"ğŸ‘¤ Pregunta: {query}\")\n",
        "print(f\"ğŸ¤– Contexto Recuperado (RAG): '{retrieved_info}'\")\n",
        "\n",
        "# --- 4. GENERACIÃ“N (LLM) ---\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Eres un asistente universitario. Responde usando SOLO el siguiente contexto.\n",
        "Contexto: {retrieved_info}\n",
        "Pregunta: {query}\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=prompt,\n",
        ")\n",
        "\n",
        "print(f\"ğŸ’¬ Respuesta Gemini: {response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycrW8x3zRTzA",
        "outputId": "a2baff48-476f-4589-b206-3b89fc481ef9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ‘¤ Pregunta: Â¿CuÃ¡ndo rindo y quÃ© nota necesito?\n",
            "ğŸ¤– Contexto Recuperado (RAG): 'Para aprobar se necesita un promedio mayor a 70.'\n",
            "ğŸ’¬ Respuesta Gemini: Para aprobar se necesita un promedio mayor a 70.\n",
            "\n",
            "No dispongo de informaciÃ³n sobre cuÃ¡ndo rindes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nDlxEf4PRUCc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}